{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt \n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2012 = pd.read_csv('data/raw/2012_Data.csv', encoding='latin1', low_memory=False)\n",
    "df_2013 = pd.read_csv('data/raw/2013_Data.csv', encoding='latin1', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the two DataFrames\n",
    "merged_data = pd.concat([df_2012, df_2013], ignore_index=True)\n",
    "\n",
    "# Display the first few rows of the merged data\n",
    "merged_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Handling Missing Values\n",
    "- Identifying Missing Data: Determine where missing values occur and why (e.g., data entry errors, collection issues).\n",
    "- Strategies for Handling Missing Data:\n",
    "    - Removal: Drop rows or columns with excessive missing values if they don’t significantly affect the analysis.\n",
    "    - Imputation: Replace missing values using techniques like mean, median, mode, or more sophisticated methods (e.g., regression or k-nearest neighbors).\n",
    "2. Correcting Inconsistent Data\n",
    "- Fix inconsistencies in data entries (e.g., different formats for the same value, such as \"USA\" and \"United States\").\n",
    "- Standardize formats for dates, phone numbers, and other data types.\n",
    "3. Removing Duplicates\n",
    "- Identify and remove duplicate records, which can skew analysis results and lead to incorrect conclusions.\n",
    "4. Handling Outliers\n",
    "- Outliers can distort analysis, especially in statistical modeling. Identify them using visualization techniques (e.g., boxplots).\n",
    "- Decide whether to remove, transform, or keep outliers based on the context and goals of the analysis.\n",
    "5. Data Type Conversion\n",
    "- Ensure that each column has the appropriate data type (e.g., numeric, categorical, datetime).\n",
    "- Convert data types if needed (e.g., parsing date strings into datetime objects).\n",
    "6. Normalization and Scaling\n",
    "- Normalization: Rescale numerical data to fit within a particular range, often [0, 1].\n",
    "- Standardization: Adjust the data to have a mean of zero and a standard deviation of one, which is important for certain machine learning algorithms.\n",
    "7. Handling Categorical Variables\n",
    "- Convert categorical variables to numeric form if required for analysis (e.g., one-hot encoding).\n",
    "- Combine similar categories to reduce complexity (e.g., grouping rare categories together).\n",
    "8. Feature Engineering\n",
    "- Creating New Features: Generate new variables based on existing ones to capture additional insights.\n",
    "- Dropping Irrelevant Features: Remove features that do not add value or are highly correlated, to avoid multicollinearity.\n",
    "9. Text Cleaning (for Textual Data)\n",
    "- For textual data, cleaning involves removing punctuation, converting to lowercase, removing stopwords, and stemming or lemmatizing words to bring them to their root form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Handling Missing Values\n",
    "- Identifying Missing Data: Determine where missing values occur and why (e.g., data entry errors, collection issues).\n",
    "- Strategies for Handling Missing Data:\n",
    "    - Removal: Drop rows or columns with excessive missing values if they don’t significantly affect the analysis.\n",
    "    - Imputation: Replace missing values using techniques like mean, median, mode, or more sophisticated methods (e.g., regression or k-nearest neighbors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the length of data and rows.\n",
    "merged_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indentifying the null values.\n",
    "merged_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The null values are shown in column 'item_source_class', which is 1,988,382, the same as the normal data length. This means this column does not use, so we decided to remove  it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove null values.\n",
    "merged_data.drop('item_source_class', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recheck null value after remove\n",
    "merged_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expected that each row in each column would be unique, as each row represents a specific order item from a customer. If there are duplicate rows in any column, it could indicate an issue with data accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the length of data and rows.\n",
    "merged_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Correcting Inconsistent Data\n",
    "- Fix inconsistencies in data entries (e.g., different formats for the same value, such as \"USA\" and \"United States\").\n",
    "- Standardize formats for dates, phone numbers, and other data types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to check whether the accounting date, fiscal year, and calendar year are consistent. However, the accounting date is not in the same standardised format as the others, so we will convert it to match the same format before checking consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'accounting_date' to datetime format\n",
    "merged_data['accounting_date'] = pd.to_datetime(merged_data['accounting_date'], format='%Y%m%d')\n",
    "\n",
    "# Create separate columns for year, month, and day from the datetime object\n",
    "merged_data['accounting_year'] = merged_data['accounting_date'].dt.year\n",
    "merged_data['accounting_month'] = merged_data['accounting_date'].dt.month\n",
    "merged_data['accounting_day'] = merged_data['accounting_date'].dt.day\n",
    "\n",
    "# Display the first few rows to verify the results\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, there are new columns: 'accounting_year', 'accounting_month', and 'accounting_day'. We will use these columns to check for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort and print unique values in ascending order for calendar year, month, and day\n",
    "print(\"Calendar Year:\", sorted(merged_data['calendar_year'].unique()))\n",
    "print(\"Calendar Month:\", sorted(merged_data['calendar_month'].unique()))\n",
    "print(\"Calendar Day:\", sorted(merged_data['calendar_day'].unique()))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Sort and print unique values in ascending order for accounting year, month, and day\n",
    "print(\"Accounting Year:\", sorted(merged_data['accounting_year'].unique()))\n",
    "print(\"Accounting Month:\", sorted(merged_data['accounting_month'].unique()))\n",
    "print(\"Accounting Day:\", sorted(merged_data['accounting_day'].unique()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that the calendar column and the accounting column are consistent. Next, we will check the consistency between the fiscal column and the calendar column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort and print unique values in ascending order for calendar year and month\n",
    "print(\"Calendar Year:\", sorted(merged_data['calendar_year'].unique()))\n",
    "print(\"Calendar Month:\", sorted(merged_data['calendar_month'].unique()))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Sort and print unique values in ascending order for fiscal year and month\n",
    "print(\"Fisical Year:\", sorted(merged_data['fiscal_year'].unique()))\n",
    "print(\"Fisical Month:\", sorted(merged_data['fiscal_month'].unique()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observed that the fiscal year and month had unique patterns compared to others. Therefore, we examined the number of months within each fiscal year and identified the specific months for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'fiscal_year' and list all unique 'fiscal_month' for each year\n",
    "fiscal_months_per_year = merged_data.groupby('fiscal_year')['fiscal_month'].apply(lambda x: sorted(x.unique()))\n",
    "\n",
    "# Display the result\n",
    "print(fiscal_months_per_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon review, we found that the fiscal year follows the financial calendar in Australia. This means that the fiscal year 2012 includes July to December 2012 and January to June 2013, while the fiscal year 2013 includes July to December 2013 and January to June 2014. In summary, the fiscal year is the same as the calendar and accounting year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we decided to fix inconsistencies in data entries for the following columns: 'order_type_code', 'abc_class_volume', 'abc_class_code', 'warehouse_code', 'environment_group_code', 'business_area_code', 'customer_district_code', and 'technology_group_code'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **order_type_code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique order type\n",
    "print(merged_data['order_type_code'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows that 'PME' does not have any list in our document, so we check how many rows show 'PME'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for rows where 'order_type_code' is 'PME'\n",
    "pme_rows = merged_data[merged_data['order_type_code'] == 'PME']\n",
    "\n",
    "# Count the number of rows\n",
    "pme_count = len(pme_rows)\n",
    "\n",
    "# Display the count\n",
    "print(f\"Number of rows with 'order_type_code' as 'PME': {pme_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It resulted in 243 rows, so we decided to drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'order_type_code' is 'PME'\n",
    "merged_data = merged_data[merged_data['order_type_code'] != 'PME']\n",
    "\n",
    "# Display the first few rows to confirm\n",
    "merged_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **abc_class_volume**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique class volume\n",
    "print(merged_data['abc_class_volume'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show consistency and a uniform format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **abc_class_code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique class code\n",
    "print(merged_data['abc_class_code'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show consistency and a uniform format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **warehouse_code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique warehouse code\n",
    "print(merged_data['warehouse_code'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that there are many warehouse codes, so we need to double-check them against our reference list to identify any discrepancies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of valid warehouse codes from our reference\n",
    "valid_warehouse_codes = [\n",
    "    'Q1', 'V0', 'S0', 'T0', 'Q0', 'N0', 'KN0', 'W0', 'S1', 'Unk', 'GS0', 'GN0', \n",
    "    'CN0', 'CQ0', 'CV0', 'GW0', 'CS0', 'CW0', 'GQ0', 'AS0', 'CT0', 'CS1', 'CN1', \n",
    "    'CZ0', 'LW0', 'LQ0', 'LS1', 'LV0', 'LN9', 'LS0', 'FWE', 'EN0', 'FW2', 'JT0', \n",
    "    'FA1', 'FWA', 'FA2', '1N0', '1S0', '5N2', '5V0', '5S0', '5W0', '1V0', '1Q0', \n",
    "    '1N1', '1W0', '1Q1', '5Q0', '5T0', '5S1', '5N1', '1T0', '1S1', '5Q1']\n",
    "\n",
    "# Filter the DataFrame to get only the invalid warehouse codes\n",
    "invalid_warehouse_data = merged_data[~merged_data['warehouse_code'].isin(valid_warehouse_codes)]\n",
    "\n",
    "# Group by the invalid warehouse codes and count their occurrences\n",
    "invalid_code_counts = invalid_warehouse_data['warehouse_code'].value_counts()\n",
    "\n",
    "# Display the grouped counts of invalid codes\n",
    "print(\"Counts of invalid warehouse codes:\")\n",
    "print(invalid_code_counts)\n",
    "len(invalid_code_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show some invalid entries compared to our reference list. However, the codes 'V0', 'N0', 'Q0', 'W0', 'S0', 'Q1', 'T0', and 'S1' are present in our list but contain extra spaces. Therefore, we need to correct their format by removing the spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary to map codes with spaces to the cleaned versions\n",
    "replacement_map = {\n",
    "    'V0 ': 'V0',\n",
    "    'N0 ': 'N0',\n",
    "    'Q0 ': 'Q0',\n",
    "    'W0 ': 'W0',\n",
    "    'S0 ': 'S0',\n",
    "    'Q1 ': 'Q1',\n",
    "    'T0 ': 'T0',\n",
    "    'S1 ': 'S1'}\n",
    "\n",
    "# Replace values in the 'warehouse_code' column based on the mapping\n",
    "merged_data['warehouse_code'] = merged_data['warehouse_code'].replace(replacement_map)\n",
    "\n",
    "# Verify the replacements\n",
    "print(\"Unique warehouse codes after replacement:\")\n",
    "print(merged_data['warehouse_code'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After cleaning, check for invalid warehouse codes again\n",
    "invalid_warehouse_data = merged_data[~merged_data['warehouse_code'].isin(valid_warehouse_codes)]\n",
    "\n",
    "# Count occurrences of each invalid warehouse code\n",
    "invalid_code_counts = invalid_warehouse_data['warehouse_code'].value_counts()\n",
    "\n",
    "# Display the counts of invalid warehouse codes\n",
    "print(\"Counts of invalid warehouse codes after cleaning:\")\n",
    "print(invalid_code_counts)\n",
    "len(invalid_warehouse_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After rechecking, we found that the 'warehouse_code' has data invalid around 215 rows, So we decided to delete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of invalid warehouse codes to drop\n",
    "invalid_codes = ['1T1', 'BB1', '1N2', '1N3']\n",
    "\n",
    "# Drop rows where 'warehouse_code' is one of the invalid codes\n",
    "merged_data = merged_data[~merged_data['warehouse_code'].isin(invalid_codes)]\n",
    "\n",
    "# Display the shape to confirm rows have been dropped\n",
    "print(merged_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **environment_group_code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unique environment column\n",
    "print(merged_data['environment_group_code'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that the environment column codes contain extra spaces, so I will replace them int correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary to map codes with spaces to the cleaned versions\n",
    "replacement_environ = {\n",
    "    'S                             ': 'S',\n",
    "    'P                             ': 'P',\n",
    "    'D                             ': 'D',\n",
    "    'Z                             ': 'Z',\n",
    "    'C                             ': 'C',\n",
    "    'M                             ': 'M',\n",
    "    'R                             ': 'R',\n",
    "    'I                             ': 'I',\n",
    "    'NA                            ': 'NA'}\n",
    "\n",
    "# Replace values in the 'environment_group_code' column based on the mapping\n",
    "merged_data['environment_group_code'] = merged_data['environment_group_code'].replace(replacement_environ)\n",
    "unique_warehouse_code = merged_data['environment_group_code'].unique()\n",
    "print(unique_warehouse_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, we checked the code against our environment group list, and no invalid entries were found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **business_area_code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the unique business code\n",
    "print(merged_data['business_area_code'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that there are many business codes, so we need to double-check them against our reference list to identify any discrepancies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of valid business codes from your reference\n",
    "business_area_codes = {\n",
    "    'LMP', 'FLD', 'OTH', 'SUR', 'COM', 'DLT', 'TRO', 'URB', 'HLB', 'SAE', 'RWY', \n",
    "    'LCP', 'PEN', 'EXL', 'TAL', '945', '950', '980', '920', '960', '910', '930', \n",
    "    '999', '970', '940', '985', 'IAE', 'IAI'}\n",
    "\n",
    "# Filter the DataFrame to get only the invalid business codes\n",
    "invalid_business_data = merged_data[~merged_data['business_area_code'].isin(business_area_codes)]\n",
    "\n",
    "# Group by the invalid business codes and count their occurrences\n",
    "invalid_business_counts = invalid_business_data['business_area_code'].value_counts()\n",
    "\n",
    "# Display the grouped counts of invalid codes\n",
    "print(\"Counts of invalid business codes:\")\n",
    "print(invalid_business_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show some invalid entries compared to our reference list. However, some of the codes are present in our list but contain extra spaces. Therefore, we need to correct their format by removing the spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary to map codes with spaces to the cleaned versions\n",
    "replacement_business_code = {\n",
    "    'LMP                           ': 'LMP',\n",
    "    'FLD                           ': 'FLD',\n",
    "    'OTH                           ': 'OTH',\n",
    "    'SUR                           ': 'SUR',\n",
    "    'COM                           ': 'COM',\n",
    "    'DLT                           ': 'DLT',\n",
    "    'TRO                           ': 'TRO',\n",
    "    'URB                           ': 'URB',\n",
    "    'HLB                           ': 'HLB',\n",
    "    'SAE                           ': 'SAE',\n",
    "    'RWY                           ': 'RWY',\n",
    "    'LCP                           ': 'LCP',\n",
    "    'PEN                           ': 'PEN',\n",
    "    'EXL                           ': 'EXL',\n",
    "    'TAL                           ': 'TAL',\n",
    "    'IAE                           ': 'IAE',\n",
    "    'IAI                           ': 'IAI'}\n",
    "\n",
    "# Replace values in the 'business_area_code' column based on the mapping\n",
    "merged_data['business_area_code'] = merged_data['business_area_code'].replace(replacement_business_code)\n",
    "\n",
    "# Verify the replacements\n",
    "print(\"Unique business codes after replacement:\")\n",
    "print(merged_data['business_area_code'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After cleaning, check for invalid business codes again\n",
    "invalid_business_data = merged_data[~merged_data['business_area_code'].isin(business_area_codes)]\n",
    "\n",
    "# Count occurrences of each invalid business code\n",
    "invalid_business_counts = invalid_business_data['business_area_code'].value_counts()\n",
    "\n",
    "# Display the counts of invalid business codes\n",
    "print(\"Counts of invalid business codes:\")\n",
    "print(invalid_business_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After rechecking, we found that the 'business_area_code' has not data invalid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **customer_district_code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the unique customer district column\n",
    "print(merged_data['customer_district_code'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that there are many customer codes, so we need to double-check them against our reference list to identify any discrepancies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of valid customer district codes from your reference\n",
    "customer_district_code = {'410', '300', '500', '310', '400', '200', '210', '720', '710', '600',\n",
    "                          '510', '530', '535', '540', '520', '545'}\n",
    "\n",
    "# Filter the DataFrame to get only the invalid customer codes\n",
    "invalid_customer_data = merged_data[~merged_data['customer_district_code'].astype(str).isin(customer_district_code)]\n",
    "\n",
    "# Group by the invalid customer codes and count their occurrences\n",
    "invalid_customer_counts = invalid_customer_data['customer_district_code'].value_counts()\n",
    "\n",
    "# Display the grouped counts of invalid codes\n",
    "print(\"Counts of invalid business codes:\")\n",
    "print(invalid_customer_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After rechecking, we found that the 'customer_district_code' has data invalid around 6 rows, So we decided to delete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of invalid customer codes to drop\n",
    "invalid_customer = ['100']\n",
    "\n",
    "# Drop rows where 'customer_district_code' is one of the invalid codes\n",
    "merged_data = merged_data[~merged_data['customer_district_code'].astype(str).isin(invalid_customer)]\n",
    "\n",
    "# Display the shape to confirm rows have been dropped\n",
    "print(merged_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After cleaning, check for invalid customer codes again\n",
    "invalid_customer_data = merged_data[~merged_data['customer_district_code'].astype(str).isin(customer_district_code)]\n",
    "\n",
    "# Count occurrences of each invalid customer code\n",
    "invalid_customer_counts = invalid_customer_data['customer_district_code'].value_counts()\n",
    "\n",
    "# Display the counts of invalid customer codes\n",
    "print(\"Counts of invalid business codes:\")\n",
    "print(invalid_customer_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **technology_group_code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the unique technology code\n",
    "print(merged_data['technology_group_code'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that there are many technology codes, so we need to double-check them against our reference list to identify any discrepancies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of valid technology codes from your reference\n",
    "technology_group_codes = [\n",
    "    'SYLV', 'NA', 'PIER', '96', '219', '92', '214', '380', '102', '123', '110', \n",
    "    '98', '213', '580', '108', '999', '198', 'SCHR', '82', '140', '540', 'AUST', \n",
    "    '480', '760', '460', '600', '520', '998', '570', '280', '330', '220', '415', \n",
    "    '340', '225', 'CSE', '565', '410', '560', '800', '680', '320', '550', '400', \n",
    "    '160', '440', '420', '500', '555', '360', '290', '780', '545', '181', '240', \n",
    "    '640', '820', '595', '300', '830', '70', '850', '86', '720', '740', '310', \n",
    "    'FGDFT', '350', '206', '64', 'PNZ', '63', 'CROM', '100', '78', '207', '208', \n",
    "    '76', '211', '205', '85', '90', '68', '130', 'GLG', 'INLIT', '72', '61', \n",
    "    '880', '217', '215', '80', '210', 'INZ', '250', '118', '66']\n",
    "\n",
    "# Filter the DataFrame to get only the invalid technology codes\n",
    "invalid_technology_data = merged_data[~merged_data['technology_group_code'].isin(technology_group_codes)]\n",
    "\n",
    "# Group by the invalid technology codes and count their occurrences\n",
    "invalid_technology_counts = invalid_technology_data['technology_group_code'].value_counts()\n",
    "\n",
    "# Display the grouped counts of invalid codes\n",
    "print(\"Counts of invalid technology codes:\")\n",
    "print(invalid_technology_counts)\n",
    "len(invalid_technology_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show some invalid entries compared to our reference list. However, some of the codes are present in our list but contain extra spaces. Therefore, we need to correct their format by removing the spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary to map codes with spaces to the cleaned versions\n",
    "replacement_technology_code = {\n",
    "    'SYLV                                    ': 'SYLV',\n",
    "    'NA                                      ': 'NA',\n",
    "    'PIER                                    ': 'PIER',\n",
    "    'SCHR                                    ': 'SCHR',\n",
    "    'AUST                                    ': 'AUST',\n",
    "    'CSE                                     ': 'CSE',\n",
    "    'FGDFT                                   ': 'FGDFT',\n",
    "    'PNZ                                     ': 'PNZ',\n",
    "    'CROM                                    ': 'CROM',\n",
    "    'GLG                                     ': 'GLG',\n",
    "    'INLIT                                   ': 'INLIT',\n",
    "    'INZ                                     ': 'INZ',\n",
    "    'PHANT                                   ': 'PHANT',\n",
    "    'DIGIN                                   ': 'DIGIN'}\n",
    "\n",
    "# Replace values in the 'technology_group_code' column based on the mapping\n",
    "merged_data['technology_group_code'] = merged_data['technology_group_code'].replace(replacement_technology_code)\n",
    "\n",
    "# Verify the replacements\n",
    "print(\"Unique technology codes after replacement:\")\n",
    "print(merged_data['technology_group_code'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After cleaning, check for invalid technology codes again\n",
    "invalid_technology_data = merged_data[~merged_data['technology_group_code'].isin(technology_group_codes)]\n",
    "\n",
    "# Count occurrences of each invalid technology code\n",
    "invalid_technology_counts = invalid_technology_data['technology_group_code'].value_counts()\n",
    "\n",
    "# Display the counts of invalid technology codes\n",
    "print(\"Counts of invalid technology codes:\")\n",
    "print(invalid_technology_counts)\n",
    "len(invalid_technology_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After rechecking, we found that the 'technology_group_code' has data invalid around 210 rows, So we decided to delete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of invalid technology codes to drop\n",
    "invalid_technology = ['128', 'DIGIN', 'PHANT', '88', '114', '112']\n",
    "\n",
    "# Drop rows where 'technology_group_code' is one of the invalid codes\n",
    "merged_data = merged_data[~merged_data['technology_group_code'].isin(invalid_technology)]\n",
    "\n",
    "# Display the shape to confirm rows have been dropped\n",
    "print(merged_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the unique currency\n",
    "print(merged_data['currency'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each unique currency\n",
    "print(merged_data['currency'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following an analysis of the currency counts, we decided to remove entries with blank spaces and standardize 'AUS' to 'AUD'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'AUS' with 'AUD'\n",
    "merged_data['currency'] = merged_data['currency'].replace('AUS', 'AUD')\n",
    "\n",
    "# Drop rows where 'currency' is blank or contains only spaces\n",
    "merged_data = merged_data[merged_data['currency'].str.strip() != '']\n",
    "\n",
    "# Verify the changes\n",
    "print(merged_data['currency'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Removing Duplicates\n",
    "- Identify and remove duplicate records, which can skew analysis results and lead to incorrect conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows based on all columns\n",
    "duplicate_rows = merged_data[merged_data.duplicated(keep='first')]\n",
    "\n",
    "# Display the duplicate rows\n",
    "print(\"Duplicate rows based on all columns:\")\n",
    "display(duplicate_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After identifying the duplicate records, it shows the duplicate records 8,209 rows, which we will remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate rows based on all columns\n",
    "cleaned_data = merged_data.drop_duplicates()\n",
    "\n",
    "# Display the shape to confirm duplicates were removed\n",
    "print(\"Data after removing duplicates:\", cleaned_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Handling Outliers\n",
    "- Outliers can distort analysis, especially in statistical modeling. Identify them using visualization techniques (e.g., boxplots).\n",
    "- Decide whether to remove, transform, or keep outliers based on the context and goals of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Focusing on value-related columns, value_sales, value_cost, value_quantity is critical because these directly impact financial performance and operational decisions.\n",
    "- Based on the output of value_price_adjustment, it seems that this column primarily contains binary values (0 and 1), with an overwhelming majority being 0. This suggests that value_price_adjustment is likely a categorical or indicator variable rather than a continuous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_data['value_price_adjustment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up the figure with a grid of 1x3 subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot the box plot for 'value_sales' in the first subplot\n",
    "merged_data['value_sales'].plot(kind='box', ax=axes[0])\n",
    "axes[0].set_title('Boxplot of value_sales')\n",
    "\n",
    "# Plot the box plot for 'value_cost' in the second subplot\n",
    "merged_data['value_cost'].plot(kind='box', ax=axes[1])\n",
    "axes[1].set_title('Boxplot of value_cost')\n",
    "\n",
    "# Plot the box plot for 'value_quantity' in the third subplot\n",
    "merged_data['value_quantity'].plot(kind='box', ax=axes[2])\n",
    "axes[2].set_title('Boxplot of value_quantity')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For value_sales\n",
    "Q1_sales = merged_data['value_sales'].quantile(0.25)\n",
    "Q3_sales = merged_data['value_sales'].quantile(0.75)\n",
    "IQR_sales = Q3_sales - Q1_sales\n",
    "lower_bound_sales = Q1_sales - 1.5 * IQR_sales\n",
    "upper_bound_sales = Q3_sales + 1.5 * IQR_sales\n",
    "print(\"Bounds for value_sales:\", lower_bound_sales, \"-\", upper_bound_sales)\n",
    "\n",
    "# For value_cost\n",
    "Q1_cost = merged_data['value_cost'].quantile(0.25)\n",
    "Q3_cost = merged_data['value_cost'].quantile(0.75)\n",
    "IQR_cost = Q3_cost - Q1_cost\n",
    "lower_bound_cost = Q1_cost - 1.5 * IQR_cost\n",
    "upper_bound_cost = Q3_cost + 1.5 * IQR_cost\n",
    "print(\"Bounds for value_cost:\", lower_bound_cost, \"-\", upper_bound_cost)\n",
    "\n",
    "# For value_quantity\n",
    "Q1_quantity = merged_data['value_quantity'].quantile(0.25)\n",
    "Q3_quantity = merged_data['value_quantity'].quantile(0.75)\n",
    "IQR_quantity = Q3_quantity - Q1_quantity\n",
    "lower_bound_quantity = Q1_quantity - 1.5 * IQR_quantity\n",
    "upper_bound_quantity = Q3_quantity + 1.5 * IQR_quantity\n",
    "print(\"Bounds for value_quantity:\", lower_bound_quantity, \"-\", upper_bound_quantity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter outliers for each column\n",
    "sales_outliers = merged_data[(merged_data['value_sales'] < lower_bound_sales) | (merged_data['value_sales'] > upper_bound_sales)]\n",
    "cost_outliers = merged_data[(merged_data['value_cost'] < lower_bound_cost) | (merged_data['value_cost'] > upper_bound_cost)]\n",
    "quantity_outliers = merged_data[(merged_data['value_quantity'] < lower_bound_quantity) | (merged_data['value_quantity'] > upper_bound_quantity)]\n",
    "\n",
    "# Display only the outliers for each column\n",
    "print(\"Sales Outliers:\\n\", sales_outliers[['value_sales']])\n",
    "print(\"Cost Outliers:\\n\", cost_outliers[['value_cost']])\n",
    "print(\"Quantity Outliers:\\n\", quantity_outliers[['value_quantity']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting thresholds 500,000 for value_sales, 600,000 for value_cost, and 50,000 for value_quantity helps us identify unusually large or small transactions based on what could be considered reasonable or typical in a business context.\n",
    "By defining these limits, you can filter out transactions that might represent rare, high-impact events or possible data errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 500,000 for value_sales, 600,000 for value_cost, and 50,000 for value_quantity is a starting point based on:\n",
    "\n",
    "- Visual Analysis of Box Plots: The plots indicated values around these thresholds as extremes.\n",
    "- Business Logic: These numbers reflect what could be a reasonable upper limit for most cases, while anything beyond might be rare and worth investigating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothetical business thresholds (modify based on real business knowledge)\n",
    "sales_threshold = 500000\n",
    "cost_threshold = 600000\n",
    "quantity_threshold = 50000\n",
    "\n",
    "\n",
    "# Filter based on business-defined thresholds\n",
    "business_sales_outliers = merged_data[(merged_data['value_sales'] > sales_threshold) | (merged_data['value_sales'] < -sales_threshold)]\n",
    "business_cost_outliers = merged_data[(merged_data['value_cost'] > cost_threshold) | (merged_data['value_cost'] < -cost_threshold)]\n",
    "business_quantity_outliers = merged_data[(merged_data['value_quantity'] > quantity_threshold) | (merged_data['value_quantity'] < -quantity_threshold)]\n",
    "\n",
    "# Display only the outlier values for each column\n",
    "\n",
    "print(\"Business Sales Outliers:\\n\", business_sales_outliers[['value_sales']])\n",
    "print(\"Business Cost Outliers:\\n\", business_cost_outliers[['value_cost']])\n",
    "print(\"Business Quantity Outliers:\\n\", business_quantity_outliers[['value_quantity']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove extreme outliers based on business-defined limits.\n",
    "- Focus the analysis on values that fall within a \"reasonable\" range, reducing the influence of extremely high or low values that might skew results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = merged_data[\n",
    "    (merged_data['value_sales'] <= sales_threshold) & (merged_data['value_sales'] >= -sales_threshold) &\n",
    "    (merged_data['value_cost'] <= cost_threshold) & (merged_data['value_cost'] >= -cost_threshold) &\n",
    "    (merged_data['value_quantity'] <= quantity_threshold) & (merged_data['value_quantity'] >= -quantity_threshold)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Boxplot of value_sales, value_cost, value_quantity after remove extreme outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with a grid of 1x3 subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot the box plot for 'value_sales' in the first subplot\n",
    "merged_data['value_sales'].plot(kind='box', ax=axes[0])\n",
    "axes[0].set_title('Boxplot of value_sales')\n",
    "\n",
    "# Plot the box plot for 'value_cost' in the second subplot\n",
    "merged_data['value_cost'].plot(kind='box', ax=axes[1])\n",
    "axes[1].set_title('Boxplot of value_cost')\n",
    "\n",
    "# Plot the box plot for 'value_quantity' in the third subplot\n",
    "merged_data['value_quantity'].plot(kind='box', ax=axes[2])\n",
    "axes[2].set_title('Boxplot of value_quantity')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms for each variable\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Histogram for value_sales\n",
    "axes[0].hist(merged_data['value_sales'], bins=30, edgecolor='black')\n",
    "axes[0].set_title('Histogram of value_sales')\n",
    "axes[0].set_xlabel('value_sales')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# Histogram for value_cost\n",
    "axes[1].hist(merged_data['value_cost'], bins=30, edgecolor='black')\n",
    "axes[1].set_title('Histogram of value_cost')\n",
    "axes[1].set_xlabel('value_cost')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "# Histogram for value_quantity\n",
    "axes[2].hist(merged_data['value_quantity'], bins=30, edgecolor='black')\n",
    "axes[2].set_title('Histogram of value_quantity')\n",
    "axes[2].set_xlabel('value_quantity')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Highly Skewed Data:\n",
    "The data for each of these columns is highly skewed, meaning there are many low or zero values and only a few high values.\n",
    "This often occurs when there is a large number of small transactions (or even no transactions in some cases), and only a few large transactions.\n",
    "2. Sparse Distribution:\n",
    "The narrow and high central bars suggest that the vast majority of values are very close to zero, with few values spreading out across the range.\n",
    "This could indicate that only a small percentage of transactions have high value_sales, value_cost, or value_quantity.\n",
    "3. Potential Need for Transformation:\n",
    "Because of the skewness, the data may benefit from a log transformation or another method to spread the values more evenly, especially if you plan to perform analyses or modeling that assume normality.\n",
    "4. Zero or Minimal Transactions:\n",
    "If there are many transactions with zero or minimal values in these columns, it might be useful to analyze these zero-value transactions separately. This could be a sign of canceled orders, unpaid invoices, or other business cases where no significant sales, cost, or quantity were recorded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Count zero values in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count zero values in each column\n",
    "zero_value_sales = (merged_data['value_sales'] == 0).sum()\n",
    "zero_value_cost = (merged_data['value_cost'] == 0).sum()\n",
    "zero_value_quantity = (merged_data['value_quantity'] == 0).sum()\n",
    "\n",
    "# Display the counts\n",
    "print(\"Number of zero values in value_sales:\", zero_value_sales)\n",
    "print(\"Number of zero values in value_cost:\", zero_value_cost)\n",
    "print(\"Number of zero values in value_quantity:\", zero_value_quantity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Type Conversion\n",
    "- Ensure that each column has the appropriate data type (e.g., numeric, categorical, datetime).\n",
    "- Convert data types if needed (e.g., parsing date strings into datetime objects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types of each column\n",
    "print(merged_data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Datetime conversion improves flexibility for time-based analysis, enables accurate date calculations, and enhances data filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['invoice_date'] = pd.to_datetime(merged_data['invoice_date'], format='%Y%m%d', errors='coerce')\n",
    "merged_data['order_date'] = pd.to_datetime(merged_data['order_date'], format='%Y%m%d', errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Categorical conversion reduces memory usage, speeds up processing, aids in machine learning, and maintains data integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\n",
    "    'customer_code', 'item_code', 'business_area_code', 'item_group_code', 'item_class_code',\n",
    "    'bonus_group_code', 'environment_group_code', 'technology_group_code', 'commission_group_code',\n",
    "    'reporting_classification', 'light_source', 'warehouse_code', 'abc_class_code', 'business_chain_l1_code',\n",
    "    'business_chain_l1_name', 'contact_method_code', 'salesperson_code', 'order_type_code', 'market_segment', \n",
    "    'currency', 'customer_order_number'\n",
    "]\n",
    "merged_data[categorical_columns] = merged_data[categorical_columns].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Normalization and Scaling\n",
    "- Normalization: Rescale numerical data to fit within a particular range, often [0, 1].\n",
    "- Standardization: Adjust the data to have a mean of zero and a standard deviation of one, which is important for certain machine learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Handling Categorical Variables\n",
    "- Convert categorical variables to numeric form if required for analysis (e.g., one-hot encoding).\n",
    "- Combine similar categories to reduce complexity (e.g., grouping rare categories together)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Engineering\n",
    "- Creating New Features: Generate new variables based on existing ones to capture additional insights.\n",
    "- Dropping Irrelevant Features: Remove features that do not add value or are highly correlated, to avoid multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Text Cleaning (for Textual Data)\n",
    "- For textual data, cleaning involves removing punctuation, converting to lowercase, removing stopwords, and stemming or lemmatizing words to bring them to their root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BUSA8000",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
